<h1 align="center">Healthcare App: API to CI CD Pipeline Builder </h1>

<p align="justify">This project shows how to take a simple data app and treat it like a real product: build it, containerize it, and deploy it automatically to Kubernetes using modern DevOps tools. It focuses on a healthcare use case so the flow feels realistic, but the pattern applies to any data or ML dashboard.</p>

## **Introduction**

<p align = "justify"></p>The core of the project is a Streamlit application that acts as a “real‑time healthcare data pipeline” dashboard. Instead of relying on static CSVs, the app pulls fresh patient‑like records from the RandomUser API and facility data from a public New York State health dataset, then generates extra synthetic healthcare data such as services, medications, costs, and dates. All of this is combined into a single view where you can inspect patient tables, summary statistics, and basic visualizations like cost distributions and time trends. The idea is to mimic how a healthcare analytics team might explore operational and clinical data in one place, without exposing real PHI.​</p>

<p align = "justify">On top of the basic UI, the app demonstrates simple “real‑time” behavior: when you click the Refresh Data button, Streamlit re‑runs the script, pulls a new sample from the APIs, regenerates the synthetic services and medications, and refreshes all tables and charts. This makes it a good teaching example for how data apps behave when they depend on live API calls and randomized logic, which is common in dashboards, simulations, and internal tools.</p>

## **Project overview**

<p align = "justify">Around this Streamlit app, the project builds a complete CI/CD and deployment story using Docker, GitHub Actions, and Kubernetes. The Dockerfile takes the Python code, installs the minimal dependencies (Streamlit, pandas, requests), and exposes the app on port 8501, which is the standard Streamlit port. This gives you a reproducible container image that will run the same way on your laptop, in a cluster, or on any server that supports containers.​</p>

<p align = "justify">The repository includes a GitHub Actions workflow that runs whenever you push to the main branch. The workflow checks out the code, uses Docker Buildx to build the image, logs into Docker Hub using encrypted secrets, and pushes an updated latest tag for your app. In other words, simply pushing code to GitHub is enough to build and publish a new container image, which is the “CI” part of the pipeline. From there, the Kubernetes manifests (a Deployment and a Service) define how many replicas of the app should run and how they are exposed over the network. The Deployment ensures two pods of the Streamlit app are always running with the published image, and the Service provides a stable IP/port and load‑balances traffic across those pods.​</p>

<p align = "justify">Because the manifests are just YAML files, the same configuration can be applied to a local Minikube cluster or a cloud‑hosted Kubernetes cluster with almost no changes. On Minikube, you can use minikube service or kubectl port-forward to access the dashboard from your browser; in the cloud, the LoadBalancer service type can give you a public URL. This combination of Streamlit, Docker, GitHub Actions, and Kubernetes turns a single Python file into a realistic DevOps project that demonstrates how to ship data applications in a repeatable, production‑style way.</p>

## **Project architecture**

<p align ="justify"> The application layer is the Streamlit + Python app in app.py. It calls external APIs to fetch patient‑like and facility data, generates synthetic healthcare services and medication records, joins everything into a single DataFrame with summary statistics, and renders tables and charts such as service cost, medication cost, counts, and time series. It also includes a “Refresh Data” button that triggers st.rerun(), causing the script to execute again and pull or generate new data on demand.</p>

<p align = "justify">The container layer is defined by the Dockerfile. It uses python:3.13 as the base image, sets /app as the working directory, copies all project files into the image, and installs only the required Python libraries (streamlit, pandas, requests). It exposes port 8501 and starts the app with streamlit run app.py --server.port=8501 --server.address=0.0.0.0, so the app behaves the same way whether it is run in local Docker, Minikube, or a cloud Kubernetes cluster.</p>

<p align = "justify">The CI/CD layer is implemented with a GitHub Actions workflow in automation.yaml. On every push to the main branch, it checks out the repository code, sets up Docker Buildx, logs into Docker Hub using encrypted secrets, and builds the Docker image from the current code. It then pushes the image to Docker Hub with the tag prathameshvarhadpande/healthcare-api-to-ci-cd-pipeline-builder:latest, ensuring that any change pushed to main automatically produces a new version of the image in the registry without manual Docker commands.</p>

<p align = "justify">The orchestration layer uses Kubernetes manifests in deployment.yaml and service.yaml. The Deployment healthcare-api-to-ci-cd-pipeline-builder-deployment runs two replicas of the Docker Hub image, labels the pods with app: myapp, exposes containerPort: 8501, and lets Kubernetes restart pods if they crash. The Service healthcare-api-to-ci-cd-pipeline-builder-service selects all pods with app: myapp, exposes port 8501 and forwards it to targetPort: 8501 on each pod, and uses type LoadBalancer, which requests a real external load balancer IP in cloud clusters or behaves like a NodePort that you access via minikube service or port‑forward in Minikube. Together, this creates a pipeline where the Streamlit app becomes a Docker image, is auto‑built to Docker Hub, is run as two replicas on Kubernetes, and is exposed to users on port 8501.</p>

## **Project Structure**

<p align = "justify">The app.py file is the heart of the project. It contains all the Python code for the Streamlit dashboard. Inside this file are functions that fetch data from external APIs (fetch_patients() for RandomUser and fetch_facilities() for the NY health facilities API), generate fake healthcare service and medication records (generate_healthcare_data() and generate_medication_data()), and clean or join this data (transform_patients(), assign_services_to_patients(), assign_medications_to_patients()). The main() function builds the actual Streamlit page: it creates the headings and sections, wires up the “Refresh Data” button using st.rerun(), and displays all tables, summary statistics, and charts in the browser.</p>

<p align = "justify">The Dockerfile explains how to turn the project into a Docker image. It starts from a Python base image, sets /app as the working directory, copies the project files into that folder, and installs the needed Python libraries. It also exposes port 8501 and defines the command to start the Streamlit server. Thanks to this file, you can run the app in a container with a single command, and it will behave the same way on any machine that has Docker.</p>

<p align = "justify">The deployment.yaml file describes how Kubernetes should run the app. It defines a Deployment that tells Kubernetes which Docker image to use, how many copies (replicas) of the app to run, and which labels to give the pods (here, app: myapp). It also specifies that the container inside each pod listens on port 8501. Kubernetes uses this file to create and manage the pods, restart them if they fail, and keep the desired number of replicas running.</p>

<p align = "justify">The service.yaml file describes how to expose the app inside and outside the cluster. It defines a Service that looks for pods with the label app: myapp (the pods created by the Deployment) and sends traffic to them. The Service exposes port 8501 and forwards it to port 8501 on each pod, and its type is LoadBalancer, which means it can provide an external access point depending on the environment. This Service acts as a stable network entry point and load‑balances traffic across the two replicas of the app.</p>

<p align = "justify">The .github/workflows/automation.yaml file configures the GitHub Actions workflow. This is the automation that runs whenever you push code to the main branch. The workflow checks out the repository, sets up Docker Buildx, logs into Docker Hub using the secrets dockerhub_username and dockerhub_password, and then builds the Docker image and pushes it to Docker Hub with the latest tag. This file is what turns a simple git push into a full CI/CD step, automatically producing an updated container image whenever the code changes.</p>

## **Setup**

<p align = "justify">To run locally with pure Python, first install the dependencies (ideally inside a virtual environment) using <mark>pip install streamlit pandas requests</mark>. Then, from the project root, start the app with <mark>streamlit run app.py</mark>. Finally, open your browser and go to <mark>http://localhost:8501</mark>. In this mode, <mark>app.py</mark> runs directly on your machine using your local Python environment, which is the simplest way to test and iterate on the code.</p>

<p align = "justify">To run the project with Docker, build the image from the project root using <mark>docker build -t healthcare-api-to-ci-cd-pipeline-builder .</mark>. After the image is built, start a container with <mark>docker run --rm -p 8501:8501 healthcare-api-to-ci-cd-pipeline-builder</mark>, which maps port <mark>8501</mark> inside the container to port <mark>8501</mark> on your host. Then open <mark>http://localhost:8501</mark> in your browser. Here Docker relies on the Dockerfile to create a reproducible image, and the Streamlit server runs inside the container instead of directly on your host OS.</p>

<p align = "justify">To build and push via GitHub Actions, first configure two secrets in your GitHub repository settings: <mark>dockerhub_username</mark> (your Docker Hub username) and <mark>dockerhub_password</mark> (your Docker Hub password or access token). After that, any time you run <mark>git add ., git commit -m "Update app", and git push origin main</mark>, the workflow defined in <mark>.github/workflows/automation.yaml</mark> will automatically start. This workflow checks out the code, sets up Docker Buildx, logs into Docker Hub with the stored secrets, builds the Docker image, and pushes <mark>prathameshvarhadpande/healthcare-api-to-ci-cd-pipeline-builder:latest</mark>. You can monitor all of these steps under the Actions tab in your GitHub repo.</p>

<p align = "justify">To run the app on Kubernetes (for example, with Minikube), begin by starting a local cluster using <mark>minikube start</mark>. From the project root, apply the Kubernetes manifests with <mark>kubectl apply -f deployment.yaml</mark> and <mark>kubectl apply -f service.yaml</mark>, then verify that the resources are running by using <mark>kubectl get pods</mark> and <mark>kubectl get svc healthcare-api-to-ci-cd-pipeline-builder-service</mark>. To access the app, either run <mark>minikube service healthcare-api-to-ci-cd-pipeline-builder-service</mark> (which prints a URL and often opens it automatically) or, alternatively, use <mark>kubectl port-forward service/healthcare-api-to-ci-cd-pipeline-builder-service 8501:8501 </mark>and then open <mark>http://localhost:8501</mark> in your browser. In this setup, <mark>deployment.yaml</mark> ensures two replicas of the Docker image are running, and <mark>service.yaml</mark> exposes them on port <mark>8501</mark>, while Minikube or port‑forwarding makes the Service reachable from your local machine.</p>
